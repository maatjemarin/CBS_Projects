This course covers the following topics:

- Basics of Natural Language Processing (NLP) such as POS-tagging, Entity recognition
- Language Modeling using N-grams
- Text classification using supervised machine learning approaches
- Unsupervised methods for NLP and latent models.
- Word-embeddings and Word Vectors
- Neural Networks for NLP and Neural Language Models
- Information Retrieval
- Relation Extraction, Question Answering, Dialog systems and Chatbots
- Web Crawling and Link Analysis


The aim of this project was to show our knowledge of and capabilities in applying different Natural Language Processing techniques from the Machine Learning domain. For this project we test the performance of four different machine learning models in classifying tweets into one of two categories: emergency tweets or non-emergency tweets. We compare a Naive Bayes Classifier, a Support Vector Machine, a Recurrent Neural Network, and a pretrained BERT model, each of which were trained on 7,613 tweets. Preprocessing steps such as tokenization, stop words removal, and lemmatization were employed in order to make the input fit for use.
